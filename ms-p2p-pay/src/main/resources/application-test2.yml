server:
  port: 9040
# ahalim: Remark unused eureka
# eureka:
#   instance:
#     preferIpAddress: true
#     instance-id: ${spring.cloud.client.ipAddress}:${server.port}
#   client:
#     serviceUrl:
#       defaultZone: http://localhost:8001/eureka/
logging:
##  file: /LOG/logs/framework/service-order/${spring.application.name}.log
  file: /LOG/p2p-pay/mylog.txt
  level: debug
spring:
  # ahalim: Remark unused zipkin
  # zipkin:
  #   base-url: http://172.17.59.215:8003/
  output:
    ansi:
      enabled: always
  devtools:
    restart:
      enabled: false
  jpa:
    database: mysql
    show-sql: true
    generate-ddl: true
    use-sql-comments: true
    ddl-auto: update
  #Hibernate ddl auto (validate|create|create-drop|update)
    hibernate:
      ddl-auto: none
      naming-strategy: org.hibernate.cfg.ImprovedNamingStrategy
    properties:
      hibernate:
        show-sql: true
        dialect: org.hibernate.dialect.MySQL5Dialect
  datasource:
    write:
      type: com.alibaba.druid.pool.DruidDataSource
      driver-class-name: com.mysql.jdbc.Driver
      url: jdbc:mysql://doit-polardb.uat.doitglotech.cloud:3306/doitp2pnew?characterEncoding=utf8&useSSL=false
      username: lender
      password: 2Z01B*NC8qOiiJ
      # 初始化大小，最小，最大
      initialSize: 5
      minIdle: 5
      maxActive: 20
      # 配置获取连接等待超时的时间
      maxWait: 60000
      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
      timeBetweenEvictionRunsMillis: 60000
      # 配置一个连接在池中的最小生存时间，单位是毫秒
      minEvictableIdleTimeMillis: 300000
      validationQuery: SELECT 1 FROM DUAL
      testWhileIdle: true
      testOnBorrow: false
      testOnReturn: false
      # 打开PSCache，并且指定每个连接上PSCache的大小
      poolPreparedStatements: true
      maxPoolPreparedStatementPerConnectionSize: 20
      # 配置监控统计拦截的filters，去掉后监控界面sql将无法统计，'wall'用于防火墙
      filters: stat,wall,log4j
      # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
      connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
        # 合并多个DruidDataSource的监控数据
        #useGlobalDataSourceStat=true
    read:
      type: com.alibaba.druid.pool.DruidDataSource
      driver-class-name: com.mysql.jdbc.Driver
      url: jdbc:mysql://doit-polardb.uat.doitglotech.cloud:3306/doitp2pnew?characterEncoding=utf8&useSSL=false
      username: lender
      password: 2Z01B*NC8qOiiJ
      # 初始化大小，最小，最大
      initialSize: 5
      minIdle: 5
      maxActive: 20
      # 配置获取连接等待超时的时间
      maxWait: 60000
      # 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒
      timeBetweenEvictionRunsMillis: 60000
      # 配置一个连接在池中的最小生存时间，单位是毫秒
      minEvictableIdleTimeMillis: 300000
      validationQuery: SELECT 1 FROM DUAL
      testWhileIdle: true
      testOnBorrow: false
      testOnReturn: false
      # 打开PSCache，并且指定每个连接上PSCache的大小
      poolPreparedStatements: true
      maxPoolPreparedStatementPerConnectionSize: 20
      # 配置监控统计拦截的filters，去掉后监控界面sql将无法统计，'wall'用于防火墙
      filters: stat,wall,log4j
      # 通过connectProperties属性来打开mergeSql功能；慢SQL记录
      connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=5000
        # 合并多个DruidDataSource的监控数据
        #useGlobalDataSourceStat=true
# ahalim: Remark unused mongo
#   data:
#     mongodb:
#       host: 172.17.59.227
#       port: 27917
#       database: demo
# #      authentication-database: admin
# #      username: root
# #      password: 123456
  redis:
    database: 5
    host: redis-A-01.uat.doitglotech.cloud
    port: 6379
    pool:
      max-active: 8
      max-wait: -1
      min-idle: 0
      max-idle: 8
    password: doit2019
  # ahalim: Remark unused kafka
  # kafka:
  #   producer:
  #     retries: 0
  #     batch-size: 16384
  #     buffer-memory: 33554432
  #     key-serializer: org.apache.kafka.common.serialization.StringSerializer
  #     defaultKey-serializer: org.apache.kafka.common.serialization.StringSerializer
  #     bootstrap-servers: 172.17.59.227:9092
  #   consumer:
  #     bootstrap-servers: 172.17.59.227:9092
  #     group-id: foo
  #     auto-offset-reset: earliest
  #     enable-auto-commit: true
  #     auto-commit-interval: 100
  #     key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
  #     defaultKey-deserializer: org.apache.kafka.common.serialization.StringDeserializer

lenderapiUrl: http://lenderapi.uat.doitglotech.cloud
third:
    pay:
       cardBinUrl: http://api.uat.doitglotech.cloud/bankcard/verify #:8888/bankcard/verify
       cardBinToken: do-it

       X-AUTH-TOKEN: eyJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJwYXltZW50IiwiZXhwIjoxODQ4ODkwMTM2fQ.LfH_9hZmU5XgBcvZOfB9Jl2Z2_FsAkg_y2Hzt2dePLBOlZCvCmFrtqzYl7xidshQp3uhR62AR0Td_KrImCEEjA
       income:
          depositUrl: http://api.uat.doitglotech.cloud/repayment/deposit # :8888/repayment/deposit
          depositConfirmUrl: http://api.uat.doitglotech.cloud/repayment/p2p/ #:8888/repayment/
#          depositChannel: DOKU
          depositChannel: CIMB
#          depositMethod: BCA
          currency: IDR
       loan:
          disburseUrl: http://api.uat.doitglotech.cloud/payment/disburse #:8888/payment/disburse
          disburseQueryUrl: http://api.uat.doitglotech.cloud/payment/ #:8888/payment/
#          disburseChannel: BCA
          currency: IDR
    doitLoan:
        url:
          doItLoanUrl: http://api.uat.doitglotech.cloud #http://47.74.157.197:9999
des:
  file:
    path: /SECRET/des-test.cer
